import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_squared_error, r2_score

df=pd.read_csv('ames_housing_no_missing.csv')
y=df['SalePrice']
X=df.drop(columns=['SalePrice'])

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

numeric_features= X_train.select_dtypes(include=['int64','float64']).columns
categorical_features=X_train.select_dtypes(include=['object']).columns

numeric_transformer=StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

# ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features)
    ]
)

models={
    "Linear":LinearRegression(),
    "Ridge":Ridge(),
    "Lasso":Lasso()
}

param_grids={
    "Linear":{},
    "Ridge":{"model__alpha":[0.01,0.1,1,10,100]},
    "Lasso":{"model__alpha":[0.01,0.1,1,10,100]}
}

results=[]

for name,model in models.items():
    pipe=Pipeline(steps=[
        ("preprocessor",preprocessor),
        ("model",model)
    ])
    grid=GridSearchCV(pipe,param_grids[name],cv=5,scoring="r2",n_jobs=-1)
    grid.fit(X_train,y_train)
    best=grid.best_estimator_
    preds=best.predict(X_test)
    results.append({
        "Model":name,
        "best params":grid.best_params_,
        "CV R2":grid.best_score_,
        "Test R2": r2_score(y_test, preds),
        "Test MSE": mean_squared_error(y_test, preds)
    })

results_df=pd.DataFrame(results)
print(results_df)

poly_pipe= Pipeline(steps=[
    ("preprocessor",preprocessor),
    ("poly",PolynomialFeatures(degree=2,include_bias=False)),
    ("model",LinearRegression())
])

poly_pipe.fit(X_train,y_train)
poly_preds=poly_pipe.predict(X_test)

results_df.loc[len(results_df)] = {
    "Model": "Polynomial (deg=2)",
    "Best Params": None,
    "CV R2": None,
    "Test R2": r2_score(y_test, poly_preds),
    "Test MSE": mean_squared_error(y_test, poly_preds)
}

results_df


best_model_name=results_df.sort_values("Test R2",ascending=False).iloc[0]["Model"]

if best_model_name == "Polynomial (deg=2)":
    best_model = poly_pipe
    y_pred = poly_preds

else:
    # retrain best model
    best_model = models[best_model_name]
    pipe = Pipeline(steps=[
        ("preprocessor", preprocessor),
        ("model", best_model)
    ])
    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)
plt.figure(figsize=(6,6))
sns.scatterplot(x=y_test, y=y_pred, alpha=0.6)
plt.plot([y_test.min(), y_test.max()],
         [y_test.min(), y_test.max()],
         'r--', lw=2)
plt.xlabel("Actual SalePrice")
plt.ylabel("Predicted SalePrice")
plt.title(f"{best_model_name} – Actual vs Predicted")
plt.show()
residuals = y_test - y_pred
plt.figure(figsize=(6,4))
sns.histplot(residuals, kde=True, bins=40)
plt.axvline(0, color='red', linestyle='--')
plt.title(f"{best_model_name} – Residual Distribution")
plt.show()
